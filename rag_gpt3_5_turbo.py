# -*- coding: utf-8 -*-
"""RAG-gpt3_5-turbo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_hIcDY9BC4nls34_wtC3bNnMALpwZfUF

### **`RAG (Retrieval-Augmented Generation) com o modelo GPT-3.5-turbo da OpenAI`**
"""

!pip install faiss-cpu

# Etapa 1: Instalar bibliotecas
from openai import OpenAI

# Etapa 2: Montar Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Etapa 3: Leitura do arquivo trn.json
import json

file_path = '/content/drive/MyDrive/FIAP/fine-tuning/projeto-fase03/trn.json'

def stream_json(file_path, limit = 5000):
  with open(file_path, 'r') as f:
    for i, line in enumerate(f):
      if i >= limit:
        break
      yield json.loads(line)

data = list(stream_json(file_path))

import re
from tqdm import tqdm
import numpy as np
import faiss


client = OpenAI(api_key = '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')

# Etapa 4: Realizando Embbedding com o modelo GPT 3.5 - turbo
def get_embedding(text, model="text-embedding-ada-002"):

  # Garantir que o texto seja uma string válida
  text = str(text).encode('utf-8', errors='replace').decode('utf-8')
  text = text.replace('\n', ' ').strip()
  text = re.sub(r'\s+', ' ', text)

  if not text:
    raise ValueError("Texto vazio para embedding.")

  print(f"Attempting to embed: {text[:200]}...")  # Para debug

  response = client.embeddings.create(
      input=text,
      model=model
  )
  return response.data[0].embedding

texts = [item['title'] for item in data if 'title' in item and item['title'].strip() != '' and 'content' in item]
contents = [item['content'] for item in data if 'title' in item and item['title'].strip() != '' and 'content' in item]

embeddings = []
for text in tqdm(texts):
    try:
        embedding = get_embedding(text)
        embeddings.append(embedding)
    except Exception as e:
        print(f"Erro ao gerar embedding para: {text[:50]}... -> {e}")

embedding_matrix = np.array(embeddings).astype('float32')

# Criar índice FAISS
index = faiss.IndexFlatL2(len(embedding_matrix[0]))
index.add(embedding_matrix)

# Etapa 5: Geração de Respostas com GPT-3.5-turbo
def gerar_resposta(pergunta, top_k=5):
  # Gerar embedding da pergunta
  pergunta_embedding = np.array(get_embedding(pergunta)).astype("float32").reshape(1, -1)

  # Buscar os top_k mais similares
  D, I = index.search(pergunta_embedding, top_k)

  # Recuperar os conteúdos correspondentes
  contexto = "\n".join([f"Título: {texts[i]}\nDescrição: {contents[i]}" for i in I[0]])

  # Montar prompt
  prompt = f"""
    Você é um assistente de IA treinado com descrições de produtos da Amazon.
    Baseado nas informações abaixo, responda à pergunta do usuário de forma clara e objetiva.

    {contexto}

    Pergunta: {pergunta}
    Resposta:
    """

  # Chamada ao GPT-3.5-turbo
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      messages=[{"role": "user", "content": prompt}],
      temperature=0.3,
      max_tokens=300
  )

  return response.choices[0].message.content

# Etapa 6: Testar o sistema
# Exemplo de pergunta
pergunta_exemplo = "What is the description of the product Misty of Chincoteague?"
resposta = gerar_resposta(pergunta_exemplo)
print("Resposta do modelo:\n", resposta)



"""### **`Fine-Tuning com GPT-2 `**

---


"""

# Etapa 1: Instalar dependências
!pip install -q transformers datasets accelerate

# Etapa 2: Carregar e preparar os dados
import pandas as pd
from datasets import Dataset


def carregar_dados(file_path, limite=5000):
    exemplos = []
    with open(file_path, 'r') as f:
        for i, linha in enumerate(f):
            if i >= limite:
                break
            item = json.loads(linha)
            if 'title' in item and 'content' in item:
                prompt = f"What is the description of the product '{item['title']}'?"
                completion = item['content']
                texto = f"{prompt} {completion}"
                exemplos.append({"text": texto})
    return exemplos

dados = carregar_dados(file_path)
df = pd.DataFrame(dados)
dataset = Dataset.from_pandas(df)

# Etapa 3: Tokenização
from transformers import GPT2Tokenizer

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(exemplo):
    return tokenizer(exemplo["text"], truncation=True, padding="max_length", max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Etapa 4: Carregar modelo GPT-2
from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained(model_name)
model.resize_token_embeddings(len(tokenizer))

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Usando dispositivo:", device)
model.to(device)

# Etapa 5: Configurar e executar o treinamento
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned-results",
    overwrite_output_dir=True,
    num_train_epochs=1,
    fp16=True,  # Treinamento com precisão mista (mais rápido em GPU)
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=2,
    prediction_loss_only=True,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none"
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

trainer.train()

# Etapa 6: Testar o modelo treinado com uma pergunta
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def gerar_resposta_gpt2(pergunta):
    entrada = tokenizer.encode_plus(
        pergunta,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=128  # aqui é o limite do input
    )
    input_ids = entrada["input_ids"].to(device)
    attention_mask = entrada["attention_mask"].to(device)

    model.eval()
    with torch.no_grad():
        saida = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=50,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.3
        )
    texto_completo = tokenizer.decode(saida[0], skip_special_tokens=True)


    # Remover o prompt da resposta
    if pergunta in texto_completo:
        resposta_limpa = texto_completo.replace(pergunta, "").strip()
    else:
        resposta_limpa = texto_completo.strip()

    return resposta_limpa

# Exemplo de pergunta
pergunta_exemplo = "What is the description of the product Echo Dot?"
resposta = gerar_resposta_gpt2(pergunta_exemplo)
print("\nResposta do modelo:\n", resposta)

